Layer Configs: initialization: random, K:400, input_layer:8, output_layer:12, temperature:1, top_k:5
===== Training VQC model =====
input_embedding_dim: 768
output_embedding_dim: 768
input shape: 81456
output shape: 81456
torch.Size([13878, 61, 768])
Initializing codebook vectors...
Total input vectors for initialization: 81456
Codebook initialized with 400 vectors

Epoch 1/50, batches: 98
  Training batch 0: loss=0.9182, recon_error=0.8875, vq_loss=0.0307, , perplexity_loss=-1.3581
  Training batch 10: loss=1.2656, recon_error=1.2542, vq_loss=0.0115, , perplexity_loss=-1.4277
  Training batch 20: loss=0.6118, recon_error=0.6005, vq_loss=0.0113, , perplexity_loss=-1.3731
  Training batch 30: loss=0.4681, recon_error=0.4576, vq_loss=0.0105, , perplexity_loss=-1.3887
  Training batch 40: loss=0.4606, recon_error=0.4503, vq_loss=0.0103, , perplexity_loss=-1.3951
  Training batch 50: loss=0.4757, recon_error=0.4654, vq_loss=0.0103, , perplexity_loss=-1.4147
  Training batch 60: loss=0.4390, recon_error=0.4286, vq_loss=0.0104, , perplexity_loss=-1.3894
  Training batch 70: loss=0.4395, recon_error=0.4294, vq_loss=0.0101, , perplexity_loss=-1.4140
  Training batch 80: loss=0.4201, recon_error=0.4100, vq_loss=0.0101, , perplexity_loss=-1.4459
  Training batch 90: loss=0.4038, recon_error=0.3936, vq_loss=0.0102, , perplexity_loss=-1.4217
  Validation batch 0: loss=0.3778, recon_error=0.3677, vq_loss=0.0101, perplexity_loss=-1.4075
  Validation batch 10: loss=0.3544, recon_error=0.3440, vq_loss=0.0103, perplexity_loss=-1.4549
Epoch 1, Train Loss: 0.692, Train Reconstruct Loss: 0.681, Train VQ Loss: 0.011, Train Perplexity Loss: -1.419, Dev Loss: 0.368, Dev Reconstruct Loss: 0.358, Dev VQ Loss: 0.010, Dev Perplexity Loss: -1.431, 
Training Perplexity: 4.284
cosine_mean_similarity: 0.697
euclidean_mean_distance: 18.882
Codebook details: 355/400 vectors used
Usage counts - Min: 0.0, Max: 25058.0
Best model updated and saved at epoch 1

Epoch 2/50, batches: 98
  Training batch 0: loss=0.3734, recon_error=0.3629, vq_loss=0.0104, , perplexity_loss=-1.4135
  Training batch 10: loss=0.3614, recon_error=0.3509, vq_loss=0.0105, , perplexity_loss=-1.5273
  Training batch 20: loss=0.1603, recon_error=0.1496, vq_loss=0.0107, , perplexity_loss=-1.6342
  Training batch 30: loss=0.1075, recon_error=0.0971, vq_loss=0.0104, , perplexity_loss=-1.6108
  Training batch 40: loss=0.0901, recon_error=0.0797, vq_loss=0.0104, , perplexity_loss=-1.6133
  Training batch 50: loss=0.0887, recon_error=0.0785, vq_loss=0.0102, , perplexity_loss=-1.7225
  Training batch 60: loss=0.0847, recon_error=0.0741, vq_loss=0.0106, , perplexity_loss=-1.7648
  Training batch 70: loss=0.0850, recon_error=0.0744, vq_loss=0.0106, , perplexity_loss=-1.8231
  Training batch 80: loss=0.0899, recon_error=0.0792, vq_loss=0.0108, , perplexity_loss=-1.8660
  Training batch 90: loss=0.0824, recon_error=0.0724, vq_loss=0.0099, , perplexity_loss=-1.8810
  Validation batch 0: loss=0.0634, recon_error=0.0536, vq_loss=0.0098, perplexity_loss=-1.9891
  Validation batch 10: loss=0.0723, recon_error=0.0622, vq_loss=0.0101, perplexity_loss=-2.0829
Epoch 2, Train Loss: 0.146, Train Reconstruct Loss: 0.135, Train VQ Loss: 0.010, Train Perplexity Loss: -1.725, Dev Loss: 0.070, Dev Reconstruct Loss: 0.060, Dev VQ Loss: 0.010, Dev Perplexity Loss: -2.021, 
Training Perplexity: 8.028
cosine_mean_similarity: 0.706
euclidean_mean_distance: 18.133
Codebook details: 321/400 vectors used
Usage counts - Min: 0.0, Max: 24255.0
Best model updated and saved at epoch 2

Epoch 3/50, batches: 98
  Training batch 0: loss=0.0763, recon_error=0.0664, vq_loss=0.0099, , perplexity_loss=-2.0483
  Training batch 10: loss=0.0782, recon_error=0.0684, vq_loss=0.0098, , perplexity_loss=-1.9419
  Training batch 20: loss=0.0871, recon_error=0.0779, vq_loss=0.0091, , perplexity_loss=-2.0258
  Training batch 30: loss=0.0848, recon_error=0.0747, vq_loss=0.0101, , perplexity_loss=-2.0001
  Training batch 40: loss=0.0750, recon_error=0.0647, vq_loss=0.0103, , perplexity_loss=-2.1732
  Training batch 50: loss=0.0749, recon_error=0.0652, vq_loss=0.0097, , perplexity_loss=-2.2504
  Training batch 60: loss=0.0736, recon_error=0.0638, vq_loss=0.0098, , perplexity_loss=-2.3740
  Training batch 70: loss=0.0883, recon_error=0.0786, vq_loss=0.0096, , perplexity_loss=-2.4109
  Training batch 80: loss=0.0707, recon_error=0.0615, vq_loss=0.0092, , perplexity_loss=-2.5206
  Training batch 90: loss=0.0844, recon_error=0.0755, vq_loss=0.0089, , perplexity_loss=-2.5998
  Validation batch 0: loss=0.0604, recon_error=0.0516, vq_loss=0.0089, perplexity_loss=-2.5639
  Validation batch 10: loss=0.0645, recon_error=0.0555, vq_loss=0.0090, perplexity_loss=-2.6341
Epoch 3, Train Loss: 0.079, Train Reconstruct Loss: 0.070, Train VQ Loss: 0.010, Train Perplexity Loss: -2.255, Dev Loss: 0.065, Dev Reconstruct Loss: 0.056, Dev VQ Loss: 0.009, Dev Perplexity Loss: -2.564, 
Training Perplexity: 13.930
cosine_mean_similarity: 0.726
euclidean_mean_distance: 16.888
Codebook details: 322/400 vectors used
Usage counts - Min: 0.0, Max: 11243.0
Best model updated and saved at epoch 3

Epoch 4/50, batches: 98
  Training batch 0: loss=0.0765, recon_error=0.0676, vq_loss=0.0089, , perplexity_loss=-2.6609
  Training batch 10: loss=0.0837, recon_error=0.0749, vq_loss=0.0089, , perplexity_loss=-2.6996
  Training batch 20: loss=0.0716, recon_error=0.0622, vq_loss=0.0094, , perplexity_loss=-2.6775
  Training batch 30: loss=0.0648, recon_error=0.0561, vq_loss=0.0086, , perplexity_loss=-2.7662
  Training batch 40: loss=0.0612, recon_error=0.0527, vq_loss=0.0085, , perplexity_loss=-2.8694
  Training batch 50: loss=0.0680, recon_error=0.0595, vq_loss=0.0085, , perplexity_loss=-2.9262
  Training batch 60: loss=0.0753, recon_error=0.0668, vq_loss=0.0084, , perplexity_loss=-3.0625
  Training batch 70: loss=0.0634, recon_error=0.0553, vq_loss=0.0082, , perplexity_loss=-3.2879
  Training batch 80: loss=0.0747, recon_error=0.0668, vq_loss=0.0079, , perplexity_loss=-3.3114
  Training batch 90: loss=0.0769, recon_error=0.0687, vq_loss=0.0082, , perplexity_loss=-3.3826
  Validation batch 0: loss=0.0571, recon_error=0.0491, vq_loss=0.0080, perplexity_loss=-3.3107
  Validation batch 10: loss=0.0601, recon_error=0.0519, vq_loss=0.0082, perplexity_loss=-3.2556
Epoch 4, Train Loss: 0.068, Train Reconstruct Loss: 0.060, Train VQ Loss: 0.008, Train Perplexity Loss: -3.017, Dev Loss: 0.061, Dev Reconstruct Loss: 0.053, Dev VQ Loss: 0.008, Dev Perplexity Loss: -3.306, 
Training Perplexity: 25.935
cosine_mean_similarity: 0.761
euclidean_mean_distance: 15.038
Codebook details: 323/400 vectors used
Usage counts - Min: 0.0, Max: 8724.0
Best model updated and saved at epoch 4

Epoch 5/50, batches: 98
  Training batch 0: loss=0.0709, recon_error=0.0628, vq_loss=0.0080, , perplexity_loss=-3.4733
  Training batch 10: loss=0.0603, recon_error=0.0526, vq_loss=0.0077, , perplexity_loss=-3.6202
  Training batch 20: loss=0.0558, recon_error=0.0480, vq_loss=0.0077, , perplexity_loss=-3.6139
  Training batch 30: loss=0.0534, recon_error=0.0458, vq_loss=0.0076, , perplexity_loss=-3.6076
  Training batch 40: loss=0.0681, recon_error=0.0606, vq_loss=0.0075, , perplexity_loss=-3.8228
  Training batch 50: loss=0.0555, recon_error=0.0477, vq_loss=0.0078, , perplexity_loss=-3.8360
  Training batch 60: loss=0.0625, recon_error=0.0545, vq_loss=0.0081, , perplexity_loss=-3.9451
  Training batch 70: loss=0.0586, recon_error=0.0507, vq_loss=0.0079, , perplexity_loss=-3.8678
  Training batch 80: loss=0.0551, recon_error=0.0476, vq_loss=0.0075, , perplexity_loss=-3.9293
  Training batch 90: loss=0.0537, recon_error=0.0464, vq_loss=0.0073, , perplexity_loss=-3.9842
  Validation batch 0: loss=0.0550, recon_error=0.0475, vq_loss=0.0075, perplexity_loss=-3.7895
  Validation batch 10: loss=0.0585, recon_error=0.0508, vq_loss=0.0077, perplexity_loss=-3.7053
Epoch 5, Train Loss: 0.060, Train Reconstruct Loss: 0.052, Train VQ Loss: 0.008, Train Perplexity Loss: -3.795, Dev Loss: 0.058, Dev Reconstruct Loss: 0.050, Dev VQ Loss: 0.008, Dev Perplexity Loss: -3.768, 
Training Perplexity: 40.662
cosine_mean_similarity: 0.793
euclidean_mean_distance: 13.353
Codebook details: 326/400 vectors used
Usage counts - Min: 0.0, Max: 4305.0
Best model updated and saved at epoch 5

Epoch 6/50, batches: 98
  Training batch 0: loss=0.0538, recon_error=0.0461, vq_loss=0.0077, , perplexity_loss=-4.0480
  Training batch 10: loss=0.0548, recon_error=0.0475, vq_loss=0.0074, , perplexity_loss=-4.0494
  Training batch 20: loss=0.0526, recon_error=0.0452, vq_loss=0.0075, , perplexity_loss=-4.0613
  Training batch 30: loss=0.0661, recon_error=0.0587, vq_loss=0.0074, , perplexity_loss=-4.0787
  Training batch 40: loss=0.0570, recon_error=0.0495, vq_loss=0.0075, , perplexity_loss=-4.0988
  Training batch 50: loss=0.0575, recon_error=0.0503, vq_loss=0.0072, , perplexity_loss=-4.0954
  Training batch 60: loss=0.0542, recon_error=0.0468, vq_loss=0.0073, , perplexity_loss=-4.2075
  Training batch 70: loss=0.0622, recon_error=0.0545, vq_loss=0.0077, , perplexity_loss=-4.1841
  Training batch 80: loss=0.0565, recon_error=0.0491, vq_loss=0.0074, , perplexity_loss=-4.1624
  Training batch 90: loss=0.0550, recon_error=0.0477, vq_loss=0.0073, , perplexity_loss=-4.2232
  Validation batch 0: loss=0.0564, recon_error=0.0491, vq_loss=0.0073, perplexity_loss=-4.0300
  Validation batch 10: loss=0.0601, recon_error=0.0527, vq_loss=0.0075, perplexity_loss=-3.9946
Epoch 6, Train Loss: 0.060, Train Reconstruct Loss: 0.052, Train VQ Loss: 0.007, Train Perplexity Loss: -4.144, Dev Loss: 0.060, Dev Reconstruct Loss: 0.053, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.049, 
Training Perplexity: 54.305
cosine_mean_similarity: 0.821
euclidean_mean_distance: 11.923
Codebook details: 325/400 vectors used
Usage counts - Min: 0.0, Max: 4820.0

Epoch 7/50, batches: 98
  Training batch 0: loss=0.0553, recon_error=0.0476, vq_loss=0.0077, , perplexity_loss=-4.3032
  Training batch 10: loss=0.0520, recon_error=0.0451, vq_loss=0.0069, , perplexity_loss=-4.2241
  Training batch 20: loss=0.0524, recon_error=0.0449, vq_loss=0.0075, , perplexity_loss=-4.4126
  Training batch 30: loss=0.0552, recon_error=0.0479, vq_loss=0.0073, , perplexity_loss=-4.3576
  Training batch 40: loss=0.0565, recon_error=0.0492, vq_loss=0.0073, , perplexity_loss=-4.4056
  Training batch 50: loss=0.0619, recon_error=0.0546, vq_loss=0.0073, , perplexity_loss=-4.2855
  Training batch 60: loss=0.0518, recon_error=0.0445, vq_loss=0.0073, , perplexity_loss=-4.4226
  Training batch 70: loss=0.0504, recon_error=0.0432, vq_loss=0.0072, , perplexity_loss=-4.3993
  Training batch 80: loss=0.0563, recon_error=0.0491, vq_loss=0.0072, , perplexity_loss=-4.4843
  Training batch 90: loss=0.0584, recon_error=0.0510, vq_loss=0.0073, , perplexity_loss=-4.4452
  Validation batch 0: loss=0.0579, recon_error=0.0507, vq_loss=0.0072, perplexity_loss=-4.2008
  Validation batch 10: loss=0.0692, recon_error=0.0618, vq_loss=0.0074, perplexity_loss=-4.1681
Epoch 7, Train Loss: 0.057, Train Reconstruct Loss: 0.049, Train VQ Loss: 0.007, Train Perplexity Loss: -4.375, Dev Loss: 0.063, Dev Reconstruct Loss: 0.055, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.199, 
Training Perplexity: 64.596
cosine_mean_similarity: 0.844
euclidean_mean_distance: 11.005
Codebook details: 306/400 vectors used
Usage counts - Min: 0.0, Max: 4803.0

Epoch 8/50, batches: 98
  Training batch 0: loss=0.0591, recon_error=0.0521, vq_loss=0.0070, , perplexity_loss=-4.3358
  Training batch 10: loss=0.0526, recon_error=0.0457, vq_loss=0.0069, , perplexity_loss=-4.4743
  Training batch 20: loss=0.0555, recon_error=0.0483, vq_loss=0.0072, , perplexity_loss=-4.5229
  Training batch 30: loss=0.0601, recon_error=0.0529, vq_loss=0.0071, , perplexity_loss=-4.4759
  Training batch 40: loss=0.0684, recon_error=0.0612, vq_loss=0.0072, , perplexity_loss=-4.5857
  Training batch 50: loss=0.0514, recon_error=0.0443, vq_loss=0.0070, , perplexity_loss=-4.5087
  Training batch 60: loss=0.0617, recon_error=0.0547, vq_loss=0.0070, , perplexity_loss=-4.5212
  Training batch 70: loss=0.0705, recon_error=0.0636, vq_loss=0.0069, , perplexity_loss=-4.5306
  Training batch 80: loss=0.0630, recon_error=0.0560, vq_loss=0.0070, , perplexity_loss=-4.4787
  Training batch 90: loss=0.0521, recon_error=0.0450, vq_loss=0.0071, , perplexity_loss=-4.6650
  Validation batch 0: loss=0.0518, recon_error=0.0448, vq_loss=0.0070, perplexity_loss=-4.3410
  Validation batch 10: loss=0.0592, recon_error=0.0520, vq_loss=0.0072, perplexity_loss=-4.2792
Epoch 8, Train Loss: 0.056, Train Reconstruct Loss: 0.049, Train VQ Loss: 0.007, Train Perplexity Loss: -4.516, Dev Loss: 0.057, Dev Reconstruct Loss: 0.050, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.312, 
Training Perplexity: 72.181
cosine_mean_similarity: 0.854
euclidean_mean_distance: 10.464
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4673.0
Best model updated and saved at epoch 8

Epoch 9/50, batches: 98
  Training batch 0: loss=0.0566, recon_error=0.0494, vq_loss=0.0072, , perplexity_loss=-4.5070
  Training batch 10: loss=0.0560, recon_error=0.0488, vq_loss=0.0072, , perplexity_loss=-4.5579
  Training batch 20: loss=0.0495, recon_error=0.0424, vq_loss=0.0071, , perplexity_loss=-4.6633
  Training batch 30: loss=0.0448, recon_error=0.0377, vq_loss=0.0071, , perplexity_loss=-4.6547
  Training batch 40: loss=0.0627, recon_error=0.0559, vq_loss=0.0068, , perplexity_loss=-4.5774
  Training batch 50: loss=0.0608, recon_error=0.0537, vq_loss=0.0070, , perplexity_loss=-4.5851
  Training batch 60: loss=0.0507, recon_error=0.0436, vq_loss=0.0071, , perplexity_loss=-4.5259
  Training batch 70: loss=0.0434, recon_error=0.0362, vq_loss=0.0072, , perplexity_loss=-4.6283
  Training batch 80: loss=0.0452, recon_error=0.0383, vq_loss=0.0068, , perplexity_loss=-4.6903
  Training batch 90: loss=0.0507, recon_error=0.0436, vq_loss=0.0070, , perplexity_loss=-4.6444
  Validation batch 0: loss=0.0507, recon_error=0.0438, vq_loss=0.0070, perplexity_loss=-4.5276
  Validation batch 10: loss=0.0621, recon_error=0.0550, vq_loss=0.0071, perplexity_loss=-4.4875
Epoch 9, Train Loss: 0.053, Train Reconstruct Loss: 0.046, Train VQ Loss: 0.007, Train Perplexity Loss: -4.610, Dev Loss: 0.055, Dev Reconstruct Loss: 0.048, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.498, 
Training Perplexity: 88.898
cosine_mean_similarity: 0.862
euclidean_mean_distance: 10.134
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4009.0
Best model updated and saved at epoch 9

Epoch 10/50, batches: 98
  Training batch 0: loss=0.0517, recon_error=0.0447, vq_loss=0.0070, , perplexity_loss=-4.6590
  Training batch 10: loss=0.0431, recon_error=0.0361, vq_loss=0.0070, , perplexity_loss=-4.7077
  Training batch 20: loss=0.0654, recon_error=0.0584, vq_loss=0.0070, , perplexity_loss=-4.6947
  Training batch 30: loss=0.0484, recon_error=0.0414, vq_loss=0.0069, , perplexity_loss=-4.6197
  Training batch 40: loss=0.0523, recon_error=0.0454, vq_loss=0.0068, , perplexity_loss=-4.6479
  Training batch 50: loss=0.0517, recon_error=0.0445, vq_loss=0.0072, , perplexity_loss=-4.6956
  Training batch 60: loss=0.0425, recon_error=0.0355, vq_loss=0.0070, , perplexity_loss=-4.7219
  Training batch 70: loss=0.0603, recon_error=0.0532, vq_loss=0.0070, , perplexity_loss=-4.6565
  Training batch 80: loss=0.0550, recon_error=0.0480, vq_loss=0.0070, , perplexity_loss=-4.6669
  Training batch 90: loss=0.0490, recon_error=0.0421, vq_loss=0.0069, , perplexity_loss=-4.6753
  Validation batch 0: loss=0.0501, recon_error=0.0433, vq_loss=0.0069, perplexity_loss=-4.4592
  Validation batch 10: loss=0.0512, recon_error=0.0441, vq_loss=0.0070, perplexity_loss=-4.4132
Epoch 10, Train Loss: 0.052, Train Reconstruct Loss: 0.045, Train VQ Loss: 0.007, Train Perplexity Loss: -4.667, Dev Loss: 0.053, Dev Reconstruct Loss: 0.046, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.430, 
Training Perplexity: 82.535
cosine_mean_similarity: 0.866
euclidean_mean_distance: 9.903
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4459.0
Best model updated and saved at epoch 10

Epoch 11/50, batches: 98
  Training batch 0: loss=0.0476, recon_error=0.0405, vq_loss=0.0071, , perplexity_loss=-4.7128
  Training batch 10: loss=0.0544, recon_error=0.0479, vq_loss=0.0065, , perplexity_loss=-4.6307
  Training batch 20: loss=0.0504, recon_error=0.0437, vq_loss=0.0067, , perplexity_loss=-4.6923
  Training batch 30: loss=0.0471, recon_error=0.0402, vq_loss=0.0069, , perplexity_loss=-4.7206
  Training batch 40: loss=0.0571, recon_error=0.0502, vq_loss=0.0070, , perplexity_loss=-4.6604
  Training batch 50: loss=0.0483, recon_error=0.0414, vq_loss=0.0069, , perplexity_loss=-4.7568
  Training batch 60: loss=0.0439, recon_error=0.0370, vq_loss=0.0069, , perplexity_loss=-4.7127
  Training batch 70: loss=0.0520, recon_error=0.0449, vq_loss=0.0072, , perplexity_loss=-4.7273
  Training batch 80: loss=0.0439, recon_error=0.0371, vq_loss=0.0069, , perplexity_loss=-4.7050
  Training batch 90: loss=0.0636, recon_error=0.0568, vq_loss=0.0068, , perplexity_loss=-4.6827
  Validation batch 0: loss=0.0585, recon_error=0.0517, vq_loss=0.0069, perplexity_loss=-4.5067
  Validation batch 10: loss=0.0701, recon_error=0.0631, vq_loss=0.0070, perplexity_loss=-4.4532
Epoch 11, Train Loss: 0.052, Train Reconstruct Loss: 0.045, Train VQ Loss: 0.007, Train Perplexity Loss: -4.705, Dev Loss: 0.068, Dev Reconstruct Loss: 0.061, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.463, 
Training Perplexity: 85.905
cosine_mean_similarity: 0.868
euclidean_mean_distance: 9.745
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4583.0

Epoch 12/50, batches: 98
  Training batch 0: loss=0.0901, recon_error=0.0831, vq_loss=0.0070, , perplexity_loss=-4.7235
  Training batch 10: loss=0.0495, recon_error=0.0425, vq_loss=0.0070, , perplexity_loss=-4.7680
  Training batch 20: loss=0.0548, recon_error=0.0476, vq_loss=0.0072, , perplexity_loss=-4.7601
  Training batch 30: loss=0.0475, recon_error=0.0408, vq_loss=0.0068, , perplexity_loss=-4.6989
  Training batch 40: loss=0.0472, recon_error=0.0406, vq_loss=0.0066, , perplexity_loss=-4.7358
  Training batch 50: loss=0.0542, recon_error=0.0478, vq_loss=0.0064, , perplexity_loss=-4.6554
  Training batch 60: loss=0.0526, recon_error=0.0459, vq_loss=0.0067, , perplexity_loss=-4.7230
  Training batch 70: loss=0.0547, recon_error=0.0478, vq_loss=0.0070, , perplexity_loss=-4.7173
  Training batch 80: loss=0.0491, recon_error=0.0424, vq_loss=0.0068, , perplexity_loss=-4.7721
  Training batch 90: loss=0.0483, recon_error=0.0416, vq_loss=0.0066, , perplexity_loss=-4.6439
  Validation batch 0: loss=0.0539, recon_error=0.0472, vq_loss=0.0068, perplexity_loss=-4.6773
  Validation batch 10: loss=0.0689, recon_error=0.0620, vq_loss=0.0069, perplexity_loss=-4.5990
Epoch 12, Train Loss: 0.051, Train Reconstruct Loss: 0.044, Train VQ Loss: 0.007, Train Perplexity Loss: -4.724, Dev Loss: 0.067, Dev Reconstruct Loss: 0.060, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.628, 
Training Perplexity: 99.380
cosine_mean_similarity: 0.869
euclidean_mean_distance: 9.637
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3989.0

Epoch 13/50, batches: 98
  Training batch 0: loss=0.0599, recon_error=0.0531, vq_loss=0.0068, , perplexity_loss=-4.7576
  Training batch 10: loss=0.0467, recon_error=0.0399, vq_loss=0.0068, , perplexity_loss=-4.7707
  Training batch 20: loss=0.0456, recon_error=0.0391, vq_loss=0.0066, , perplexity_loss=-4.7074
  Training batch 30: loss=0.0410, recon_error=0.0344, vq_loss=0.0066, , perplexity_loss=-4.7738
  Training batch 40: loss=0.0685, recon_error=0.0620, vq_loss=0.0065, , perplexity_loss=-4.7089
  Training batch 50: loss=0.0603, recon_error=0.0533, vq_loss=0.0070, , perplexity_loss=-4.7485
  Training batch 60: loss=0.0494, recon_error=0.0428, vq_loss=0.0066, , perplexity_loss=-4.6604
  Training batch 70: loss=0.0531, recon_error=0.0466, vq_loss=0.0065, , perplexity_loss=-4.6327
  Training batch 80: loss=0.0531, recon_error=0.0465, vq_loss=0.0066, , perplexity_loss=-4.7164
  Training batch 90: loss=0.0426, recon_error=0.0359, vq_loss=0.0067, , perplexity_loss=-4.7164
  Validation batch 0: loss=0.0456, recon_error=0.0389, vq_loss=0.0067, perplexity_loss=-4.5507
  Validation batch 10: loss=0.0514, recon_error=0.0445, vq_loss=0.0069, perplexity_loss=-4.4949
Epoch 13, Train Loss: 0.051, Train Reconstruct Loss: 0.044, Train VQ Loss: 0.007, Train Perplexity Loss: -4.748, Dev Loss: 0.051, Dev Reconstruct Loss: 0.044, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.502, 
Training Perplexity: 89.555
cosine_mean_similarity: 0.870
euclidean_mean_distance: 9.551
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4496.0
Best model updated and saved at epoch 13

Epoch 14/50, batches: 98
  Training batch 0: loss=0.0466, recon_error=0.0400, vq_loss=0.0066, , perplexity_loss=-4.7857
  Training batch 10: loss=0.0437, recon_error=0.0370, vq_loss=0.0067, , perplexity_loss=-4.7975
  Training batch 20: loss=0.0403, recon_error=0.0337, vq_loss=0.0066, , perplexity_loss=-4.7190
  Training batch 30: loss=0.0500, recon_error=0.0430, vq_loss=0.0070, , perplexity_loss=-4.8032
  Training batch 40: loss=0.0383, recon_error=0.0314, vq_loss=0.0069, , perplexity_loss=-4.8041
  Training batch 50: loss=0.0695, recon_error=0.0625, vq_loss=0.0070, , perplexity_loss=-4.7624
  Training batch 60: loss=0.0467, recon_error=0.0400, vq_loss=0.0067, , perplexity_loss=-4.7662
  Training batch 70: loss=0.0412, recon_error=0.0341, vq_loss=0.0071, , perplexity_loss=-4.8256
  Training batch 80: loss=0.0439, recon_error=0.0373, vq_loss=0.0066, , perplexity_loss=-4.7755
  Training batch 90: loss=0.0504, recon_error=0.0434, vq_loss=0.0071, , perplexity_loss=-4.8192
  Validation batch 0: loss=0.0481, recon_error=0.0414, vq_loss=0.0067, perplexity_loss=-4.5525
  Validation batch 10: loss=0.0549, recon_error=0.0481, vq_loss=0.0068, perplexity_loss=-4.5064
Epoch 14, Train Loss: 0.047, Train Reconstruct Loss: 0.041, Train VQ Loss: 0.007, Train Perplexity Loss: -4.764, Dev Loss: 0.052, Dev Reconstruct Loss: 0.045, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.515, 
Training Perplexity: 90.594
cosine_mean_similarity: 0.872
euclidean_mean_distance: 9.458
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4571.0

Epoch 15/50, batches: 98
  Training batch 0: loss=0.0416, recon_error=0.0350, vq_loss=0.0066, , perplexity_loss=-4.8350
  Training batch 10: loss=0.0447, recon_error=0.0382, vq_loss=0.0066, , perplexity_loss=-4.6857
  Training batch 20: loss=0.0447, recon_error=0.0383, vq_loss=0.0065, , perplexity_loss=-4.7664
  Training batch 30: loss=0.0434, recon_error=0.0368, vq_loss=0.0066, , perplexity_loss=-4.7227
  Training batch 40: loss=0.0396, recon_error=0.0331, vq_loss=0.0066, , perplexity_loss=-4.8002
  Training batch 50: loss=0.0492, recon_error=0.0425, vq_loss=0.0067, , perplexity_loss=-4.8191
  Training batch 60: loss=0.0616, recon_error=0.0548, vq_loss=0.0068, , perplexity_loss=-4.8220
  Training batch 70: loss=0.0527, recon_error=0.0460, vq_loss=0.0067, , perplexity_loss=-4.8113
  Training batch 80: loss=0.0546, recon_error=0.0478, vq_loss=0.0068, , perplexity_loss=-4.8492
  Training batch 90: loss=0.0476, recon_error=0.0407, vq_loss=0.0069, , perplexity_loss=-4.7986
  Validation batch 0: loss=0.0445, recon_error=0.0378, vq_loss=0.0066, perplexity_loss=-4.6310
  Validation batch 10: loss=0.0546, recon_error=0.0478, vq_loss=0.0068, perplexity_loss=-4.5642
Epoch 15, Train Loss: 0.049, Train Reconstruct Loss: 0.042, Train VQ Loss: 0.007, Train Perplexity Loss: -4.781, Dev Loss: 0.050, Dev Reconstruct Loss: 0.043, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.580, 
Training Perplexity: 95.986
cosine_mean_similarity: 0.873
euclidean_mean_distance: 9.385
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4333.0
Best model updated and saved at epoch 15

Epoch 16/50, batches: 98
  Training batch 0: loss=0.0545, recon_error=0.0478, vq_loss=0.0067, , perplexity_loss=-4.7551
  Training batch 10: loss=0.0521, recon_error=0.0453, vq_loss=0.0068, , perplexity_loss=-4.8180
  Training batch 20: loss=0.0525, recon_error=0.0456, vq_loss=0.0068, , perplexity_loss=-4.8157
  Training batch 30: loss=0.0413, recon_error=0.0347, vq_loss=0.0067, , perplexity_loss=-4.7653
  Training batch 40: loss=0.0495, recon_error=0.0424, vq_loss=0.0071, , perplexity_loss=-4.8888
  Training batch 50: loss=0.0560, recon_error=0.0496, vq_loss=0.0064, , perplexity_loss=-4.7686
  Training batch 60: loss=0.0405, recon_error=0.0336, vq_loss=0.0068, , perplexity_loss=-4.8117
  Training batch 70: loss=0.0427, recon_error=0.0361, vq_loss=0.0067, , perplexity_loss=-4.7145
  Training batch 80: loss=0.0585, recon_error=0.0517, vq_loss=0.0068, , perplexity_loss=-4.8050
  Training batch 90: loss=0.0419, recon_error=0.0352, vq_loss=0.0067, , perplexity_loss=-4.7803
  Validation batch 0: loss=0.0473, recon_error=0.0408, vq_loss=0.0066, perplexity_loss=-4.6091
  Validation batch 10: loss=0.0473, recon_error=0.0406, vq_loss=0.0067, perplexity_loss=-4.5610
Epoch 16, Train Loss: 0.046, Train Reconstruct Loss: 0.040, Train VQ Loss: 0.007, Train Perplexity Loss: -4.790, Dev Loss: 0.050, Dev Reconstruct Loss: 0.044, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.580, 
Training Perplexity: 95.675
cosine_mean_similarity: 0.875
euclidean_mean_distance: 9.311
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4437.0

Epoch 17/50, batches: 98
  Training batch 0: loss=0.0527, recon_error=0.0460, vq_loss=0.0067, , perplexity_loss=-4.7855
  Training batch 10: loss=0.0479, recon_error=0.0412, vq_loss=0.0067, , perplexity_loss=-4.8288
  Training batch 20: loss=0.0421, recon_error=0.0358, vq_loss=0.0063, , perplexity_loss=-4.7636
  Training batch 30: loss=0.0480, recon_error=0.0413, vq_loss=0.0067, , perplexity_loss=-4.7843
  Training batch 40: loss=0.0583, recon_error=0.0520, vq_loss=0.0063, , perplexity_loss=-4.7387
  Training batch 50: loss=0.0398, recon_error=0.0330, vq_loss=0.0068, , perplexity_loss=-4.8884
  Training batch 60: loss=0.0415, recon_error=0.0350, vq_loss=0.0066, , perplexity_loss=-4.8795
  Training batch 70: loss=0.0456, recon_error=0.0390, vq_loss=0.0066, , perplexity_loss=-4.7724
  Training batch 80: loss=0.0457, recon_error=0.0393, vq_loss=0.0064, , perplexity_loss=-4.7765
  Training batch 90: loss=0.0463, recon_error=0.0395, vq_loss=0.0068, , perplexity_loss=-4.8536
  Validation batch 0: loss=0.0457, recon_error=0.0392, vq_loss=0.0065, perplexity_loss=-4.5868
  Validation batch 10: loss=0.0464, recon_error=0.0397, vq_loss=0.0067, perplexity_loss=-4.5641
Epoch 17, Train Loss: 0.046, Train Reconstruct Loss: 0.039, Train VQ Loss: 0.007, Train Perplexity Loss: -4.799, Dev Loss: 0.049, Dev Reconstruct Loss: 0.042, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.579, 
Training Perplexity: 95.971
cosine_mean_similarity: 0.876
euclidean_mean_distance: 9.254
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4504.0
Best model updated and saved at epoch 17

Epoch 18/50, batches: 98
  Training batch 0: loss=0.0430, recon_error=0.0366, vq_loss=0.0064, , perplexity_loss=-4.7481
  Training batch 10: loss=0.0487, recon_error=0.0421, vq_loss=0.0066, , perplexity_loss=-4.8128
  Training batch 20: loss=0.0463, recon_error=0.0395, vq_loss=0.0068, , perplexity_loss=-4.8615
  Training batch 30: loss=0.0419, recon_error=0.0354, vq_loss=0.0065, , perplexity_loss=-4.8364
  Training batch 40: loss=0.0473, recon_error=0.0411, vq_loss=0.0063, , perplexity_loss=-4.7784
  Training batch 50: loss=0.0394, recon_error=0.0328, vq_loss=0.0066, , perplexity_loss=-4.8190
  Training batch 60: loss=0.0425, recon_error=0.0357, vq_loss=0.0068, , perplexity_loss=-4.8542
  Training batch 70: loss=0.0442, recon_error=0.0378, vq_loss=0.0064, , perplexity_loss=-4.8173
  Training batch 80: loss=0.0466, recon_error=0.0399, vq_loss=0.0066, , perplexity_loss=-4.8198
  Training batch 90: loss=0.0467, recon_error=0.0403, vq_loss=0.0064, , perplexity_loss=-4.7630
  Validation batch 0: loss=0.0466, recon_error=0.0402, vq_loss=0.0065, perplexity_loss=-4.6551
  Validation batch 10: loss=0.0442, recon_error=0.0376, vq_loss=0.0066, perplexity_loss=-4.5913
Epoch 18, Train Loss: 0.045, Train Reconstruct Loss: 0.038, Train VQ Loss: 0.007, Train Perplexity Loss: -4.803, Dev Loss: 0.047, Dev Reconstruct Loss: 0.040, Dev VQ Loss: 0.007, Dev Perplexity Loss: -4.619, 
Training Perplexity: 98.620
cosine_mean_similarity: 0.877
euclidean_mean_distance: 9.203
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4396.0
Best model updated and saved at epoch 18

Epoch 19/50, batches: 98
  Training batch 0: loss=0.0480, recon_error=0.0413, vq_loss=0.0067, , perplexity_loss=-4.8282
  Training batch 10: loss=0.0480, recon_error=0.0414, vq_loss=0.0066, , perplexity_loss=-4.8442
  Training batch 20: loss=0.0491, recon_error=0.0426, vq_loss=0.0065, , perplexity_loss=-4.8154
  Training batch 30: loss=0.0427, recon_error=0.0367, vq_loss=0.0060, , perplexity_loss=-4.6387
  Training batch 40: loss=0.0469, recon_error=0.0403, vq_loss=0.0066, , perplexity_loss=-4.8605
  Training batch 50: loss=0.0484, recon_error=0.0418, vq_loss=0.0066, , perplexity_loss=-4.9074
  Training batch 60: loss=0.0431, recon_error=0.0368, vq_loss=0.0063, , perplexity_loss=-4.7871
  Training batch 70: loss=0.0500, recon_error=0.0436, vq_loss=0.0064, , perplexity_loss=-4.7622
  Training batch 80: loss=0.0507, recon_error=0.0441, vq_loss=0.0066, , perplexity_loss=-4.8577
  Training batch 90: loss=0.0447, recon_error=0.0381, vq_loss=0.0066, , perplexity_loss=-4.8631
  Validation batch 0: loss=0.0454, recon_error=0.0390, vq_loss=0.0064, perplexity_loss=-4.6271
  Validation batch 10: loss=0.0543, recon_error=0.0477, vq_loss=0.0066, perplexity_loss=-4.5294
Epoch 19, Train Loss: 0.047, Train Reconstruct Loss: 0.040, Train VQ Loss: 0.006, Train Perplexity Loss: -4.811, Dev Loss: 0.050, Dev Reconstruct Loss: 0.044, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.570, 
Training Perplexity: 92.698
cosine_mean_similarity: 0.877
euclidean_mean_distance: 9.162
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4552.0

Epoch 20/50, batches: 98
  Training batch 0: loss=0.0514, recon_error=0.0451, vq_loss=0.0063, , perplexity_loss=-4.8180
  Training batch 10: loss=0.0440, recon_error=0.0375, vq_loss=0.0065, , perplexity_loss=-4.8260
  Training batch 20: loss=0.0371, recon_error=0.0311, vq_loss=0.0061, , perplexity_loss=-4.7952
  Training batch 30: loss=0.0476, recon_error=0.0412, vq_loss=0.0064, , perplexity_loss=-4.8299
  Training batch 40: loss=0.0485, recon_error=0.0422, vq_loss=0.0063, , perplexity_loss=-4.8032
  Training batch 50: loss=0.0393, recon_error=0.0329, vq_loss=0.0064, , perplexity_loss=-4.8663
  Training batch 60: loss=0.0398, recon_error=0.0336, vq_loss=0.0062, , perplexity_loss=-4.8243
  Training batch 70: loss=0.0381, recon_error=0.0322, vq_loss=0.0059, , perplexity_loss=-4.7414
  Training batch 80: loss=0.0574, recon_error=0.0511, vq_loss=0.0063, , perplexity_loss=-4.8135
  Training batch 90: loss=0.0507, recon_error=0.0445, vq_loss=0.0062, , perplexity_loss=-4.8259
  Validation batch 0: loss=0.0482, recon_error=0.0418, vq_loss=0.0064, perplexity_loss=-4.7787
  Validation batch 10: loss=0.0497, recon_error=0.0432, vq_loss=0.0065, perplexity_loss=-4.6999
Epoch 20, Train Loss: 0.045, Train Reconstruct Loss: 0.038, Train VQ Loss: 0.006, Train Perplexity Loss: -4.818, Dev Loss: 0.050, Dev Reconstruct Loss: 0.044, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.739, 
Training Perplexity: 109.937
cosine_mean_similarity: 0.878
euclidean_mean_distance: 9.121
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3805.0

Epoch 21/50, batches: 98
  Training batch 0: loss=0.0497, recon_error=0.0434, vq_loss=0.0063, , perplexity_loss=-4.7975
  Training batch 10: loss=0.0438, recon_error=0.0374, vq_loss=0.0063, , perplexity_loss=-4.7864
  Training batch 20: loss=0.0489, recon_error=0.0425, vq_loss=0.0064, , perplexity_loss=-4.7800
  Training batch 30: loss=0.0448, recon_error=0.0387, vq_loss=0.0061, , perplexity_loss=-4.8360
  Training batch 40: loss=0.0409, recon_error=0.0345, vq_loss=0.0064, , perplexity_loss=-4.7704
  Training batch 50: loss=0.0453, recon_error=0.0391, vq_loss=0.0063, , perplexity_loss=-4.8409
  Training batch 60: loss=0.0494, recon_error=0.0433, vq_loss=0.0062, , perplexity_loss=-4.7475
  Training batch 70: loss=0.0576, recon_error=0.0516, vq_loss=0.0059, , perplexity_loss=-4.7264
  Training batch 80: loss=0.0471, recon_error=0.0410, vq_loss=0.0061, , perplexity_loss=-4.8039
  Training batch 90: loss=0.0427, recon_error=0.0363, vq_loss=0.0064, , perplexity_loss=-4.8543
  Validation batch 0: loss=0.0447, recon_error=0.0384, vq_loss=0.0063, perplexity_loss=-4.6226
  Validation batch 10: loss=0.0440, recon_error=0.0376, vq_loss=0.0064, perplexity_loss=-4.5606
Epoch 21, Train Loss: 0.044, Train Reconstruct Loss: 0.038, Train VQ Loss: 0.006, Train Perplexity Loss: -4.821, Dev Loss: 0.048, Dev Reconstruct Loss: 0.041, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.608, 
Training Perplexity: 95.642
cosine_mean_similarity: 0.878
euclidean_mean_distance: 9.074
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4401.0

Epoch 22/50, batches: 98
  Training batch 0: loss=0.0430, recon_error=0.0368, vq_loss=0.0062, , perplexity_loss=-4.8168
  Training batch 10: loss=0.0402, recon_error=0.0339, vq_loss=0.0063, , perplexity_loss=-4.7609
  Training batch 20: loss=0.0396, recon_error=0.0333, vq_loss=0.0062, , perplexity_loss=-4.8046
  Training batch 30: loss=0.0487, recon_error=0.0423, vq_loss=0.0064, , perplexity_loss=-4.8783
  Training batch 40: loss=0.0414, recon_error=0.0353, vq_loss=0.0061, , perplexity_loss=-4.7561
  Training batch 50: loss=0.0378, recon_error=0.0316, vq_loss=0.0062, , perplexity_loss=-4.8389
  Training batch 60: loss=0.0411, recon_error=0.0350, vq_loss=0.0060, , perplexity_loss=-4.7893
  Training batch 70: loss=0.0408, recon_error=0.0342, vq_loss=0.0066, , perplexity_loss=-4.9194
  Training batch 80: loss=0.0371, recon_error=0.0304, vq_loss=0.0067, , perplexity_loss=-4.8734
  Training batch 90: loss=0.0353, recon_error=0.0291, vq_loss=0.0062, , perplexity_loss=-4.8902
  Validation batch 0: loss=0.0422, recon_error=0.0360, vq_loss=0.0062, perplexity_loss=-4.7570
  Validation batch 10: loss=0.0430, recon_error=0.0367, vq_loss=0.0064, perplexity_loss=-4.6700
Epoch 22, Train Loss: 0.043, Train Reconstruct Loss: 0.036, Train VQ Loss: 0.006, Train Perplexity Loss: -4.825, Dev Loss: 0.044, Dev Reconstruct Loss: 0.038, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.731, 
Training Perplexity: 106.701
cosine_mean_similarity: 0.879
euclidean_mean_distance: 9.030
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3900.0
Best model updated and saved at epoch 22

Epoch 23/50, batches: 98
  Training batch 0: loss=0.0399, recon_error=0.0338, vq_loss=0.0061, , perplexity_loss=-4.8587
  Training batch 10: loss=0.0434, recon_error=0.0372, vq_loss=0.0062, , perplexity_loss=-4.8659
  Training batch 20: loss=0.0594, recon_error=0.0536, vq_loss=0.0058, , perplexity_loss=-4.7132
  Training batch 30: loss=0.0614, recon_error=0.0552, vq_loss=0.0062, , perplexity_loss=-4.7745
  Training batch 40: loss=0.0599, recon_error=0.0538, vq_loss=0.0061, , perplexity_loss=-4.8090
  Training batch 50: loss=0.0550, recon_error=0.0487, vq_loss=0.0063, , perplexity_loss=-4.8942
  Training batch 60: loss=0.0525, recon_error=0.0461, vq_loss=0.0064, , perplexity_loss=-4.8544
  Training batch 70: loss=0.0531, recon_error=0.0470, vq_loss=0.0061, , perplexity_loss=-4.8080
  Training batch 80: loss=0.0475, recon_error=0.0411, vq_loss=0.0064, , perplexity_loss=-4.8493
  Training batch 90: loss=0.0440, recon_error=0.0380, vq_loss=0.0060, , perplexity_loss=-4.8084
  Validation batch 0: loss=0.0441, recon_error=0.0379, vq_loss=0.0062, perplexity_loss=-4.6134
  Validation batch 10: loss=0.0468, recon_error=0.0404, vq_loss=0.0063, perplexity_loss=-4.5538
Epoch 23, Train Loss: 0.054, Train Reconstruct Loss: 0.048, Train VQ Loss: 0.006, Train Perplexity Loss: -4.828, Dev Loss: 0.047, Dev Reconstruct Loss: 0.040, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.589, 
Training Perplexity: 94.988
cosine_mean_similarity: 0.879
euclidean_mean_distance: 8.990
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4521.0

Epoch 24/50, batches: 98
  Training batch 0: loss=0.0434, recon_error=0.0374, vq_loss=0.0060, , perplexity_loss=-4.7883
  Training batch 10: loss=0.0396, recon_error=0.0334, vq_loss=0.0062, , perplexity_loss=-4.8236
  Training batch 20: loss=0.0515, recon_error=0.0454, vq_loss=0.0062, , perplexity_loss=-4.8561
  Training batch 30: loss=0.0423, recon_error=0.0361, vq_loss=0.0061, , perplexity_loss=-4.8032
  Training batch 40: loss=0.0416, recon_error=0.0352, vq_loss=0.0064, , perplexity_loss=-4.8942
  Training batch 50: loss=0.0443, recon_error=0.0382, vq_loss=0.0061, , perplexity_loss=-4.8285
  Training batch 60: loss=0.0442, recon_error=0.0380, vq_loss=0.0062, , perplexity_loss=-4.8224
  Training batch 70: loss=0.0448, recon_error=0.0386, vq_loss=0.0062, , perplexity_loss=-4.7889
  Training batch 80: loss=0.0493, recon_error=0.0434, vq_loss=0.0059, , perplexity_loss=-4.7509
  Training batch 90: loss=0.0450, recon_error=0.0388, vq_loss=0.0062, , perplexity_loss=-4.8993
  Validation batch 0: loss=0.0463, recon_error=0.0402, vq_loss=0.0061, perplexity_loss=-4.7484
  Validation batch 10: loss=0.0458, recon_error=0.0396, vq_loss=0.0063, perplexity_loss=-4.6871
Epoch 24, Train Loss: 0.044, Train Reconstruct Loss: 0.038, Train VQ Loss: 0.006, Train Perplexity Loss: -4.832, Dev Loss: 0.048, Dev Reconstruct Loss: 0.042, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.741, 
Training Perplexity: 108.537
cosine_mean_similarity: 0.880
euclidean_mean_distance: 8.938
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4016.0

Epoch 25/50, batches: 98
  Training batch 0: loss=0.0377, recon_error=0.0318, vq_loss=0.0060, , perplexity_loss=-4.8002
  Training batch 10: loss=0.0402, recon_error=0.0342, vq_loss=0.0060, , perplexity_loss=-4.8063
  Training batch 20: loss=0.0436, recon_error=0.0376, vq_loss=0.0061, , perplexity_loss=-4.7982
  Training batch 30: loss=0.0419, recon_error=0.0355, vq_loss=0.0064, , perplexity_loss=-4.9068
  Training batch 40: loss=0.0436, recon_error=0.0374, vq_loss=0.0062, , perplexity_loss=-4.8406
  Training batch 50: loss=0.0458, recon_error=0.0399, vq_loss=0.0059, , perplexity_loss=-4.7111
  Training batch 60: loss=0.0457, recon_error=0.0397, vq_loss=0.0059, , perplexity_loss=-4.8456
  Training batch 70: loss=0.0447, recon_error=0.0387, vq_loss=0.0060, , perplexity_loss=-4.8385
  Training batch 80: loss=0.0434, recon_error=0.0374, vq_loss=0.0060, , perplexity_loss=-4.8067
  Training batch 90: loss=0.0578, recon_error=0.0514, vq_loss=0.0064, , perplexity_loss=-4.9296
  Validation batch 0: loss=0.0440, recon_error=0.0380, vq_loss=0.0061, perplexity_loss=-4.7691
  Validation batch 10: loss=0.0459, recon_error=0.0397, vq_loss=0.0062, perplexity_loss=-4.6935
Epoch 25, Train Loss: 0.043, Train Reconstruct Loss: 0.037, Train VQ Loss: 0.006, Train Perplexity Loss: -4.828, Dev Loss: 0.047, Dev Reconstruct Loss: 0.040, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.751, 
Training Perplexity: 109.235
cosine_mean_similarity: 0.881
euclidean_mean_distance: 8.894
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3813.0

Epoch 26/50, batches: 98
  Training batch 0: loss=0.0431, recon_error=0.0372, vq_loss=0.0059, , perplexity_loss=-4.7329
  Training batch 10: loss=0.0429, recon_error=0.0373, vq_loss=0.0057, , perplexity_loss=-4.7726
  Training batch 20: loss=0.0495, recon_error=0.0434, vq_loss=0.0061, , perplexity_loss=-4.7989
  Training batch 30: loss=0.0477, recon_error=0.0418, vq_loss=0.0058, , perplexity_loss=-4.7938
  Training batch 40: loss=0.0414, recon_error=0.0355, vq_loss=0.0060, , perplexity_loss=-4.7958
  Training batch 50: loss=0.0432, recon_error=0.0375, vq_loss=0.0057, , perplexity_loss=-4.7654
  Training batch 60: loss=0.0432, recon_error=0.0371, vq_loss=0.0061, , perplexity_loss=-4.8250
  Training batch 70: loss=0.0407, recon_error=0.0346, vq_loss=0.0062, , perplexity_loss=-4.9000
  Training batch 80: loss=0.0410, recon_error=0.0351, vq_loss=0.0059, , perplexity_loss=-4.8256
  Training batch 90: loss=0.0450, recon_error=0.0388, vq_loss=0.0061, , perplexity_loss=-4.8991
  Validation batch 0: loss=0.0450, recon_error=0.0390, vq_loss=0.0060, perplexity_loss=-4.7544
  Validation batch 10: loss=0.0410, recon_error=0.0348, vq_loss=0.0061, perplexity_loss=-4.6641
Epoch 26, Train Loss: 0.043, Train Reconstruct Loss: 0.037, Train VQ Loss: 0.006, Train Perplexity Loss: -4.835, Dev Loss: 0.044, Dev Reconstruct Loss: 0.038, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.732, 
Training Perplexity: 106.070
cosine_mean_similarity: 0.882
euclidean_mean_distance: 8.843
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3990.0
Best model updated and saved at epoch 26

Epoch 27/50, batches: 98
  Training batch 0: loss=0.0437, recon_error=0.0377, vq_loss=0.0060, , perplexity_loss=-4.7834
  Training batch 10: loss=0.0416, recon_error=0.0359, vq_loss=0.0057, , perplexity_loss=-4.8427
  Training batch 20: loss=0.0395, recon_error=0.0335, vq_loss=0.0060, , perplexity_loss=-4.8762
  Training batch 30: loss=0.0512, recon_error=0.0453, vq_loss=0.0059, , perplexity_loss=-4.8430
  Training batch 40: loss=0.0423, recon_error=0.0366, vq_loss=0.0057, , perplexity_loss=-4.7444
  Training batch 50: loss=0.0524, recon_error=0.0465, vq_loss=0.0059, , perplexity_loss=-4.8027
  Training batch 60: loss=0.0402, recon_error=0.0343, vq_loss=0.0059, , perplexity_loss=-4.8084
  Training batch 70: loss=0.0420, recon_error=0.0362, vq_loss=0.0057, , perplexity_loss=-4.8474
  Training batch 80: loss=0.0388, recon_error=0.0327, vq_loss=0.0060, , perplexity_loss=-4.8135
  Training batch 90: loss=0.0427, recon_error=0.0369, vq_loss=0.0058, , perplexity_loss=-4.7344
  Validation batch 0: loss=0.0431, recon_error=0.0371, vq_loss=0.0059, perplexity_loss=-4.7743
  Validation batch 10: loss=0.0417, recon_error=0.0357, vq_loss=0.0061, perplexity_loss=-4.6990
Epoch 27, Train Loss: 0.042, Train Reconstruct Loss: 0.037, Train VQ Loss: 0.006, Train Perplexity Loss: -4.836, Dev Loss: 0.044, Dev Reconstruct Loss: 0.038, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.755, 
Training Perplexity: 109.837
cosine_mean_similarity: 0.882
euclidean_mean_distance: 8.797
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3916.0

Epoch 28/50, batches: 98
  Training batch 0: loss=0.0434, recon_error=0.0375, vq_loss=0.0059, , perplexity_loss=-4.8260
  Training batch 10: loss=0.0408, recon_error=0.0350, vq_loss=0.0058, , perplexity_loss=-4.7897
  Training batch 20: loss=0.0391, recon_error=0.0329, vq_loss=0.0063, , perplexity_loss=-4.8936
  Training batch 30: loss=0.0405, recon_error=0.0346, vq_loss=0.0059, , perplexity_loss=-4.8628
  Training batch 40: loss=0.0414, recon_error=0.0355, vq_loss=0.0059, , perplexity_loss=-4.8109
  Training batch 50: loss=0.0390, recon_error=0.0329, vq_loss=0.0061, , perplexity_loss=-4.8662
  Training batch 60: loss=0.0485, recon_error=0.0428, vq_loss=0.0057, , perplexity_loss=-4.8089
  Training batch 70: loss=0.0453, recon_error=0.0395, vq_loss=0.0058, , perplexity_loss=-4.8485
  Training batch 80: loss=0.0495, recon_error=0.0436, vq_loss=0.0059, , perplexity_loss=-4.8733
  Training batch 90: loss=0.0457, recon_error=0.0397, vq_loss=0.0060, , perplexity_loss=-4.8978
  Validation batch 0: loss=0.0454, recon_error=0.0396, vq_loss=0.0059, perplexity_loss=-4.7671
  Validation batch 10: loss=0.0468, recon_error=0.0408, vq_loss=0.0060, perplexity_loss=-4.6614
Epoch 28, Train Loss: 0.041, Train Reconstruct Loss: 0.036, Train VQ Loss: 0.006, Train Perplexity Loss: -4.842, Dev Loss: 0.047, Dev Reconstruct Loss: 0.041, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.733, 
Training Perplexity: 105.783
cosine_mean_similarity: 0.883
euclidean_mean_distance: 8.754
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4025.0

Epoch 29/50, batches: 98
  Training batch 0: loss=0.0463, recon_error=0.0405, vq_loss=0.0058, , perplexity_loss=-4.8197
  Training batch 10: loss=0.0453, recon_error=0.0395, vq_loss=0.0058, , perplexity_loss=-4.8282
  Training batch 20: loss=0.0516, recon_error=0.0456, vq_loss=0.0060, , perplexity_loss=-4.9038
  Training batch 30: loss=0.0444, recon_error=0.0385, vq_loss=0.0058, , perplexity_loss=-4.8250
  Training batch 40: loss=0.0433, recon_error=0.0375, vq_loss=0.0058, , perplexity_loss=-4.9044
  Training batch 50: loss=0.0411, recon_error=0.0352, vq_loss=0.0060, , perplexity_loss=-4.8778
  Training batch 60: loss=0.0403, recon_error=0.0343, vq_loss=0.0060, , perplexity_loss=-4.8453
  Training batch 70: loss=0.0403, recon_error=0.0343, vq_loss=0.0060, , perplexity_loss=-4.8685
  Training batch 80: loss=0.0410, recon_error=0.0351, vq_loss=0.0059, , perplexity_loss=-4.8339
  Training batch 90: loss=0.0392, recon_error=0.0333, vq_loss=0.0058, , perplexity_loss=-4.8919
  Validation batch 0: loss=0.0418, recon_error=0.0360, vq_loss=0.0058, perplexity_loss=-4.7270
  Validation batch 10: loss=0.0454, recon_error=0.0394, vq_loss=0.0059, perplexity_loss=-4.6527
Epoch 29, Train Loss: 0.043, Train Reconstruct Loss: 0.037, Train VQ Loss: 0.006, Train Perplexity Loss: -4.839, Dev Loss: 0.044, Dev Reconstruct Loss: 0.038, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.719, 
Training Perplexity: 104.870
cosine_mean_similarity: 0.882
euclidean_mean_distance: 8.710
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3913.0
Best model updated and saved at epoch 29

Epoch 30/50, batches: 98
  Training batch 0: loss=0.0425, recon_error=0.0368, vq_loss=0.0057, , perplexity_loss=-4.8426
  Training batch 10: loss=0.0395, recon_error=0.0339, vq_loss=0.0056, , perplexity_loss=-4.8141
  Training batch 20: loss=0.0388, recon_error=0.0332, vq_loss=0.0056, , perplexity_loss=-4.8215
  Training batch 30: loss=0.0437, recon_error=0.0380, vq_loss=0.0057, , perplexity_loss=-4.8315
  Training batch 40: loss=0.0566, recon_error=0.0507, vq_loss=0.0060, , perplexity_loss=-4.8339
  Training batch 50: loss=0.0426, recon_error=0.0371, vq_loss=0.0056, , perplexity_loss=-4.7345
  Training batch 60: loss=0.0418, recon_error=0.0361, vq_loss=0.0057, , perplexity_loss=-4.7696
  Training batch 70: loss=0.0402, recon_error=0.0346, vq_loss=0.0056, , perplexity_loss=-4.8292
  Training batch 80: loss=0.0431, recon_error=0.0372, vq_loss=0.0059, , perplexity_loss=-4.8705
  Training batch 90: loss=0.0435, recon_error=0.0375, vq_loss=0.0060, , perplexity_loss=-4.8669
  Validation batch 0: loss=0.0440, recon_error=0.0383, vq_loss=0.0057, perplexity_loss=-4.6994
  Validation batch 10: loss=0.0440, recon_error=0.0382, vq_loss=0.0059, perplexity_loss=-4.6058
Epoch 30, Train Loss: 0.042, Train Reconstruct Loss: 0.037, Train VQ Loss: 0.006, Train Perplexity Loss: -4.840, Dev Loss: 0.047, Dev Reconstruct Loss: 0.042, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.669, 
Training Perplexity: 100.062
cosine_mean_similarity: 0.883
euclidean_mean_distance: 8.658
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4256.0

Epoch 31/50, batches: 98
  Training batch 0: loss=0.0608, recon_error=0.0551, vq_loss=0.0056, , perplexity_loss=-4.7952
  Training batch 10: loss=0.0402, recon_error=0.0346, vq_loss=0.0056, , perplexity_loss=-4.8504
  Training batch 20: loss=0.0395, recon_error=0.0340, vq_loss=0.0054, , perplexity_loss=-4.7960
  Training batch 30: loss=0.0421, recon_error=0.0362, vq_loss=0.0059, , perplexity_loss=-4.8629
  Training batch 40: loss=0.0362, recon_error=0.0307, vq_loss=0.0056, , perplexity_loss=-4.8603
  Training batch 50: loss=0.0436, recon_error=0.0378, vq_loss=0.0058, , perplexity_loss=-4.9139
  Training batch 60: loss=0.0481, recon_error=0.0423, vq_loss=0.0058, , perplexity_loss=-4.8496
  Training batch 70: loss=0.0527, recon_error=0.0470, vq_loss=0.0057, , perplexity_loss=-4.7913
  Training batch 80: loss=0.0434, recon_error=0.0374, vq_loss=0.0060, , perplexity_loss=-4.8967
  Training batch 90: loss=0.0423, recon_error=0.0363, vq_loss=0.0060, , perplexity_loss=-4.9365
  Validation batch 0: loss=0.0435, recon_error=0.0378, vq_loss=0.0056, perplexity_loss=-4.7194
  Validation batch 10: loss=0.0411, recon_error=0.0353, vq_loss=0.0058, perplexity_loss=-4.6164
Epoch 31, Train Loss: 0.042, Train Reconstruct Loss: 0.036, Train VQ Loss: 0.006, Train Perplexity Loss: -4.840, Dev Loss: 0.043, Dev Reconstruct Loss: 0.037, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.688, 
Training Perplexity: 101.126
cosine_mean_similarity: 0.883
euclidean_mean_distance: 8.607
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4138.0
Best model updated and saved at epoch 31

Epoch 32/50, batches: 98
  Training batch 0: loss=0.0424, recon_error=0.0367, vq_loss=0.0057, , perplexity_loss=-4.8732
  Training batch 10: loss=0.0389, recon_error=0.0332, vq_loss=0.0056, , perplexity_loss=-4.8311
  Training batch 20: loss=0.0488, recon_error=0.0433, vq_loss=0.0055, , perplexity_loss=-4.7986
  Training batch 30: loss=0.0470, recon_error=0.0410, vq_loss=0.0060, , perplexity_loss=-4.8706
  Training batch 40: loss=0.0426, recon_error=0.0368, vq_loss=0.0057, , perplexity_loss=-4.8543
  Training batch 50: loss=0.0452, recon_error=0.0395, vq_loss=0.0056, , perplexity_loss=-4.8601
  Training batch 60: loss=0.0391, recon_error=0.0334, vq_loss=0.0057, , perplexity_loss=-4.8586
  Training batch 70: loss=0.0471, recon_error=0.0414, vq_loss=0.0056, , perplexity_loss=-4.8246
  Training batch 80: loss=0.0355, recon_error=0.0302, vq_loss=0.0053, , perplexity_loss=-4.7165
  Training batch 90: loss=0.0419, recon_error=0.0365, vq_loss=0.0055, , perplexity_loss=-4.7814
  Validation batch 0: loss=0.0410, recon_error=0.0355, vq_loss=0.0056, perplexity_loss=-4.7956
  Validation batch 10: loss=0.0396, recon_error=0.0339, vq_loss=0.0057, perplexity_loss=-4.6953
Epoch 32, Train Loss: 0.040, Train Reconstruct Loss: 0.035, Train VQ Loss: 0.006, Train Perplexity Loss: -4.840, Dev Loss: 0.042, Dev Reconstruct Loss: 0.036, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.762, 
Training Perplexity: 109.428
cosine_mean_similarity: 0.884
euclidean_mean_distance: 8.555
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3775.0
Best model updated and saved at epoch 32

Epoch 33/50, batches: 98
  Training batch 0: loss=0.0385, recon_error=0.0329, vq_loss=0.0056, , perplexity_loss=-4.9005
  Training batch 10: loss=0.0408, recon_error=0.0352, vq_loss=0.0056, , perplexity_loss=-4.8941
  Training batch 20: loss=0.0433, recon_error=0.0376, vq_loss=0.0057, , perplexity_loss=-4.8385
  Training batch 30: loss=0.0443, recon_error=0.0387, vq_loss=0.0056, , perplexity_loss=-4.8837
  Training batch 40: loss=0.0364, recon_error=0.0307, vq_loss=0.0056, , perplexity_loss=-4.8343
  Training batch 50: loss=0.0438, recon_error=0.0383, vq_loss=0.0055, , perplexity_loss=-4.8355
  Training batch 60: loss=0.0426, recon_error=0.0374, vq_loss=0.0053, , perplexity_loss=-4.7365
  Training batch 70: loss=0.0423, recon_error=0.0367, vq_loss=0.0056, , perplexity_loss=-4.8828
  Training batch 80: loss=0.0371, recon_error=0.0315, vq_loss=0.0056, , perplexity_loss=-4.9205
  Training batch 90: loss=0.0408, recon_error=0.0351, vq_loss=0.0056, , perplexity_loss=-4.9168
  Validation batch 0: loss=0.0464, recon_error=0.0409, vq_loss=0.0055, perplexity_loss=-4.6997
  Validation batch 10: loss=0.0604, recon_error=0.0548, vq_loss=0.0056, perplexity_loss=-4.5870
Epoch 33, Train Loss: 0.042, Train Reconstruct Loss: 0.036, Train VQ Loss: 0.006, Train Perplexity Loss: -4.844, Dev Loss: 0.059, Dev Reconstruct Loss: 0.053, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.657, 
Training Perplexity: 98.202
cosine_mean_similarity: 0.884
euclidean_mean_distance: 8.500
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4324.0

Epoch 34/50, batches: 98
  Training batch 0: loss=0.0477, recon_error=0.0422, vq_loss=0.0054, , perplexity_loss=-4.8289
  Training batch 10: loss=0.0543, recon_error=0.0487, vq_loss=0.0056, , perplexity_loss=-4.9417
  Training batch 20: loss=0.0401, recon_error=0.0348, vq_loss=0.0054, , perplexity_loss=-4.8509
  Training batch 30: loss=0.0451, recon_error=0.0396, vq_loss=0.0055, , perplexity_loss=-4.8501
  Training batch 40: loss=0.0428, recon_error=0.0373, vq_loss=0.0055, , perplexity_loss=-4.7956
  Training batch 50: loss=0.0457, recon_error=0.0402, vq_loss=0.0055, , perplexity_loss=-4.8204
  Training batch 60: loss=0.0426, recon_error=0.0369, vq_loss=0.0057, , perplexity_loss=-4.8757
  Training batch 70: loss=0.0455, recon_error=0.0400, vq_loss=0.0055, , perplexity_loss=-4.8204
  Training batch 80: loss=0.0396, recon_error=0.0339, vq_loss=0.0057, , perplexity_loss=-4.9449
  Training batch 90: loss=0.0429, recon_error=0.0373, vq_loss=0.0056, , perplexity_loss=-4.8750
  Validation batch 0: loss=0.0428, recon_error=0.0373, vq_loss=0.0054, perplexity_loss=-4.8174
  Validation batch 10: loss=0.0464, recon_error=0.0408, vq_loss=0.0056, perplexity_loss=-4.6794
Epoch 34, Train Loss: 0.045, Train Reconstruct Loss: 0.040, Train VQ Loss: 0.006, Train Perplexity Loss: -4.841, Dev Loss: 0.046, Dev Reconstruct Loss: 0.040, Dev VQ Loss: 0.006, Dev Perplexity Loss: -4.760, 
Training Perplexity: 107.703
cosine_mean_similarity: 0.883
euclidean_mean_distance: 8.447
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4007.0

Epoch 35/50, batches: 98
  Training batch 0: loss=0.0469, recon_error=0.0416, vq_loss=0.0053, , perplexity_loss=-4.8497
  Training batch 10: loss=0.0387, recon_error=0.0333, vq_loss=0.0054, , perplexity_loss=-4.7957
  Training batch 20: loss=0.0425, recon_error=0.0372, vq_loss=0.0053, , perplexity_loss=-4.8087
  Training batch 30: loss=0.0380, recon_error=0.0325, vq_loss=0.0054, , perplexity_loss=-4.8240
  Training batch 40: loss=0.0315, recon_error=0.0263, vq_loss=0.0052, , perplexity_loss=-4.8477
  Training batch 50: loss=0.0388, recon_error=0.0335, vq_loss=0.0053, , perplexity_loss=-4.8262
  Training batch 60: loss=0.0362, recon_error=0.0308, vq_loss=0.0054, , perplexity_loss=-4.7913
  Training batch 70: loss=0.0386, recon_error=0.0332, vq_loss=0.0054, , perplexity_loss=-4.7965
  Training batch 80: loss=0.0426, recon_error=0.0370, vq_loss=0.0057, , perplexity_loss=-4.8715
  Training batch 90: loss=0.0397, recon_error=0.0344, vq_loss=0.0053, , perplexity_loss=-4.8333
  Validation batch 0: loss=0.0442, recon_error=0.0388, vq_loss=0.0054, perplexity_loss=-4.6272
  Validation batch 10: loss=0.0418, recon_error=0.0363, vq_loss=0.0055, perplexity_loss=-4.5438
Epoch 35, Train Loss: 0.040, Train Reconstruct Loss: 0.035, Train VQ Loss: 0.005, Train Perplexity Loss: -4.839, Dev Loss: 0.045, Dev Reconstruct Loss: 0.039, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.600, 
Training Perplexity: 94.048
cosine_mean_similarity: 0.884
euclidean_mean_distance: 8.398
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4613.0

Epoch 36/50, batches: 98
  Training batch 0: loss=0.0421, recon_error=0.0366, vq_loss=0.0054, , perplexity_loss=-4.8604
  Training batch 10: loss=0.0478, recon_error=0.0423, vq_loss=0.0055, , perplexity_loss=-4.8010
  Training batch 20: loss=0.0421, recon_error=0.0366, vq_loss=0.0055, , perplexity_loss=-4.8995
  Training batch 30: loss=0.0400, recon_error=0.0344, vq_loss=0.0056, , perplexity_loss=-4.8735
  Training batch 40: loss=0.0527, recon_error=0.0473, vq_loss=0.0054, , perplexity_loss=-4.8081
  Training batch 50: loss=0.0427, recon_error=0.0374, vq_loss=0.0053, , perplexity_loss=-4.8348
  Training batch 60: loss=0.0454, recon_error=0.0401, vq_loss=0.0053, , perplexity_loss=-4.7917
  Training batch 70: loss=0.0372, recon_error=0.0319, vq_loss=0.0052, , perplexity_loss=-4.8414
  Training batch 80: loss=0.0351, recon_error=0.0297, vq_loss=0.0054, , perplexity_loss=-4.9030
  Training batch 90: loss=0.0421, recon_error=0.0367, vq_loss=0.0054, , perplexity_loss=-4.8769
  Validation batch 0: loss=0.0424, recon_error=0.0371, vq_loss=0.0053, perplexity_loss=-4.7354
  Validation batch 10: loss=0.0410, recon_error=0.0356, vq_loss=0.0054, perplexity_loss=-4.6062
Epoch 36, Train Loss: 0.040, Train Reconstruct Loss: 0.035, Train VQ Loss: 0.005, Train Perplexity Loss: -4.843, Dev Loss: 0.044, Dev Reconstruct Loss: 0.038, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.669, 
Training Perplexity: 100.102
cosine_mean_similarity: 0.885
euclidean_mean_distance: 8.339
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4341.0

Epoch 37/50, batches: 98
  Training batch 0: loss=0.0362, recon_error=0.0309, vq_loss=0.0053, , perplexity_loss=-4.8924
  Training batch 10: loss=0.0466, recon_error=0.0412, vq_loss=0.0054, , perplexity_loss=-4.8347
  Training batch 20: loss=0.0455, recon_error=0.0399, vq_loss=0.0056, , perplexity_loss=-4.8762
  Training batch 30: loss=0.0408, recon_error=0.0357, vq_loss=0.0052, , perplexity_loss=-4.7372
  Training batch 40: loss=0.0375, recon_error=0.0322, vq_loss=0.0053, , perplexity_loss=-4.8473
  Training batch 50: loss=0.0383, recon_error=0.0330, vq_loss=0.0054, , perplexity_loss=-4.9084
  Training batch 60: loss=0.0534, recon_error=0.0485, vq_loss=0.0049, , perplexity_loss=-4.6554
  Training batch 70: loss=0.0417, recon_error=0.0366, vq_loss=0.0050, , perplexity_loss=-4.7952
  Training batch 80: loss=0.0405, recon_error=0.0352, vq_loss=0.0053, , perplexity_loss=-4.8008
  Training batch 90: loss=0.0484, recon_error=0.0430, vq_loss=0.0054, , perplexity_loss=-4.8757
  Validation batch 0: loss=0.0560, recon_error=0.0507, vq_loss=0.0053, perplexity_loss=-4.6175
  Validation batch 10: loss=0.0700, recon_error=0.0645, vq_loss=0.0054, perplexity_loss=-4.5110
Epoch 37, Train Loss: 0.042, Train Reconstruct Loss: 0.036, Train VQ Loss: 0.005, Train Perplexity Loss: -4.843, Dev Loss: 0.071, Dev Reconstruct Loss: 0.065, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.580, 
Training Perplexity: 91.016
cosine_mean_similarity: 0.885
euclidean_mean_distance: 8.283
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4563.0

Epoch 38/50, batches: 98
  Training batch 0: loss=0.0680, recon_error=0.0628, vq_loss=0.0052, , perplexity_loss=-4.8056
  Training batch 10: loss=0.0514, recon_error=0.0463, vq_loss=0.0051, , perplexity_loss=-4.7507
  Training batch 20: loss=0.0691, recon_error=0.0637, vq_loss=0.0053, , perplexity_loss=-4.8461
  Training batch 30: loss=0.0481, recon_error=0.0430, vq_loss=0.0051, , perplexity_loss=-4.8032
  Training batch 40: loss=0.0536, recon_error=0.0482, vq_loss=0.0053, , perplexity_loss=-4.8867
  Training batch 50: loss=0.0516, recon_error=0.0460, vq_loss=0.0056, , perplexity_loss=-4.8455
  Training batch 60: loss=0.0445, recon_error=0.0392, vq_loss=0.0053, , perplexity_loss=-4.8794
  Training batch 70: loss=0.0424, recon_error=0.0371, vq_loss=0.0053, , perplexity_loss=-4.8727
  Training batch 80: loss=0.0386, recon_error=0.0334, vq_loss=0.0052, , perplexity_loss=-4.8839
  Training batch 90: loss=0.0460, recon_error=0.0406, vq_loss=0.0053, , perplexity_loss=-4.8731
  Validation batch 0: loss=0.0425, recon_error=0.0374, vq_loss=0.0052, perplexity_loss=-4.6608
  Validation batch 10: loss=0.0442, recon_error=0.0389, vq_loss=0.0053, perplexity_loss=-4.5570
Epoch 38, Train Loss: 0.047, Train Reconstruct Loss: 0.041, Train VQ Loss: 0.005, Train Perplexity Loss: -4.847, Dev Loss: 0.044, Dev Reconstruct Loss: 0.039, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.614, 
Training Perplexity: 95.298
cosine_mean_similarity: 0.883
euclidean_mean_distance: 8.213
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4512.0

Epoch 39/50, batches: 98
  Training batch 0: loss=0.0372, recon_error=0.0321, vq_loss=0.0051, , perplexity_loss=-4.8952
  Training batch 10: loss=0.0411, recon_error=0.0360, vq_loss=0.0051, , perplexity_loss=-4.8509
  Training batch 20: loss=0.0340, recon_error=0.0289, vq_loss=0.0051, , perplexity_loss=-4.8250
  Training batch 30: loss=0.0421, recon_error=0.0370, vq_loss=0.0051, , perplexity_loss=-4.8724
  Training batch 40: loss=0.0427, recon_error=0.0375, vq_loss=0.0052, , perplexity_loss=-4.9046
  Training batch 50: loss=0.0409, recon_error=0.0358, vq_loss=0.0051, , perplexity_loss=-4.8094
  Training batch 60: loss=0.0382, recon_error=0.0330, vq_loss=0.0052, , perplexity_loss=-4.7780
  Training batch 70: loss=0.0406, recon_error=0.0354, vq_loss=0.0052, , perplexity_loss=-4.8162
  Training batch 80: loss=0.0422, recon_error=0.0369, vq_loss=0.0053, , perplexity_loss=-4.9408
  Training batch 90: loss=0.0435, recon_error=0.0383, vq_loss=0.0052, , perplexity_loss=-4.8511
  Validation batch 0: loss=0.0402, recon_error=0.0350, vq_loss=0.0051, perplexity_loss=-4.7543
  Validation batch 10: loss=0.0425, recon_error=0.0372, vq_loss=0.0053, perplexity_loss=-4.6487
Epoch 39, Train Loss: 0.042, Train Reconstruct Loss: 0.036, Train VQ Loss: 0.005, Train Perplexity Loss: -4.847, Dev Loss: 0.042, Dev Reconstruct Loss: 0.037, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.709, 
Training Perplexity: 104.450
cosine_mean_similarity: 0.884
euclidean_mean_distance: 8.164
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3921.0

Epoch 40/50, batches: 98
  Training batch 0: loss=0.0364, recon_error=0.0314, vq_loss=0.0050, , perplexity_loss=-4.7641
  Training batch 10: loss=0.0396, recon_error=0.0346, vq_loss=0.0049, , perplexity_loss=-4.8147
  Training batch 20: loss=0.0362, recon_error=0.0309, vq_loss=0.0053, , perplexity_loss=-4.9028
  Training batch 30: loss=0.0407, recon_error=0.0354, vq_loss=0.0052, , perplexity_loss=-4.8380
  Training batch 40: loss=0.0392, recon_error=0.0339, vq_loss=0.0053, , perplexity_loss=-4.9347
  Training batch 50: loss=0.0447, recon_error=0.0396, vq_loss=0.0050, , perplexity_loss=-4.8041
  Training batch 60: loss=0.0389, recon_error=0.0336, vq_loss=0.0053, , perplexity_loss=-4.9039
  Training batch 70: loss=0.0349, recon_error=0.0297, vq_loss=0.0052, , perplexity_loss=-4.9329
  Training batch 80: loss=0.0369, recon_error=0.0318, vq_loss=0.0052, , perplexity_loss=-4.8881
  Training batch 90: loss=0.0435, recon_error=0.0382, vq_loss=0.0052, , perplexity_loss=-4.8530
  Validation batch 0: loss=0.0400, recon_error=0.0349, vq_loss=0.0051, perplexity_loss=-4.7353
  Validation batch 10: loss=0.0397, recon_error=0.0345, vq_loss=0.0052, perplexity_loss=-4.6483
Epoch 40, Train Loss: 0.039, Train Reconstruct Loss: 0.034, Train VQ Loss: 0.005, Train Perplexity Loss: -4.849, Dev Loss: 0.041, Dev Reconstruct Loss: 0.036, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.703, 
Training Perplexity: 104.405
cosine_mean_similarity: 0.885
euclidean_mean_distance: 8.122
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4055.0
Best model updated and saved at epoch 40

Epoch 41/50, batches: 98
  Training batch 0: loss=0.0345, recon_error=0.0294, vq_loss=0.0050, , perplexity_loss=-4.8453
  Training batch 10: loss=0.0357, recon_error=0.0305, vq_loss=0.0052, , perplexity_loss=-4.9183
  Training batch 20: loss=0.0349, recon_error=0.0300, vq_loss=0.0049, , perplexity_loss=-4.8072
  Training batch 30: loss=0.0377, recon_error=0.0327, vq_loss=0.0050, , perplexity_loss=-4.8378
  Training batch 40: loss=0.0448, recon_error=0.0400, vq_loss=0.0048, , perplexity_loss=-4.7343
  Training batch 50: loss=0.0388, recon_error=0.0336, vq_loss=0.0052, , perplexity_loss=-4.9264
  Training batch 60: loss=0.0372, recon_error=0.0322, vq_loss=0.0050, , perplexity_loss=-4.7877
  Training batch 70: loss=0.0396, recon_error=0.0345, vq_loss=0.0052, , perplexity_loss=-4.8417
  Training batch 80: loss=0.0377, recon_error=0.0326, vq_loss=0.0051, , perplexity_loss=-4.8788
  Training batch 90: loss=0.0370, recon_error=0.0320, vq_loss=0.0051, , perplexity_loss=-4.8074
  Validation batch 0: loss=0.0399, recon_error=0.0348, vq_loss=0.0051, perplexity_loss=-4.8098
  Validation batch 10: loss=0.0383, recon_error=0.0332, vq_loss=0.0052, perplexity_loss=-4.7302
Epoch 41, Train Loss: 0.038, Train Reconstruct Loss: 0.033, Train VQ Loss: 0.005, Train Perplexity Loss: -4.850, Dev Loss: 0.041, Dev Reconstruct Loss: 0.036, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.774, 
Training Perplexity: 113.320
cosine_mean_similarity: 0.886
euclidean_mean_distance: 8.088
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3831.0

Epoch 42/50, batches: 98
  Training batch 0: loss=0.0389, recon_error=0.0340, vq_loss=0.0049, , perplexity_loss=-4.7994
  Training batch 10: loss=0.0406, recon_error=0.0356, vq_loss=0.0050, , perplexity_loss=-4.8600
  Training batch 20: loss=0.0331, recon_error=0.0279, vq_loss=0.0052, , perplexity_loss=-4.8796
  Training batch 30: loss=0.0415, recon_error=0.0365, vq_loss=0.0050, , perplexity_loss=-4.8497
  Training batch 40: loss=0.0361, recon_error=0.0310, vq_loss=0.0051, , perplexity_loss=-4.8064
  Training batch 50: loss=0.0395, recon_error=0.0345, vq_loss=0.0050, , perplexity_loss=-4.8499
  Training batch 60: loss=0.0387, recon_error=0.0335, vq_loss=0.0051, , perplexity_loss=-4.7946
  Training batch 70: loss=0.0370, recon_error=0.0319, vq_loss=0.0051, , perplexity_loss=-4.9033
  Training batch 80: loss=0.0399, recon_error=0.0351, vq_loss=0.0048, , perplexity_loss=-4.7742
  Training batch 90: loss=0.0374, recon_error=0.0322, vq_loss=0.0052, , perplexity_loss=-4.8894
  Validation batch 0: loss=0.0395, recon_error=0.0345, vq_loss=0.0050, perplexity_loss=-4.8183
  Validation batch 10: loss=0.0376, recon_error=0.0325, vq_loss=0.0051, perplexity_loss=-4.7458
Epoch 42, Train Loss: 0.038, Train Reconstruct Loss: 0.033, Train VQ Loss: 0.005, Train Perplexity Loss: -4.855, Dev Loss: 0.040, Dev Reconstruct Loss: 0.035, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.791, 
Training Perplexity: 115.096
cosine_mean_similarity: 0.887
euclidean_mean_distance: 8.057
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3821.0
Best model updated and saved at epoch 42

Epoch 43/50, batches: 98
  Training batch 0: loss=0.0358, recon_error=0.0308, vq_loss=0.0050, , perplexity_loss=-4.8465
  Training batch 10: loss=0.0400, recon_error=0.0349, vq_loss=0.0050, , perplexity_loss=-4.8951
  Training batch 20: loss=0.0358, recon_error=0.0307, vq_loss=0.0051, , perplexity_loss=-4.8849
  Training batch 30: loss=0.0358, recon_error=0.0308, vq_loss=0.0050, , perplexity_loss=-4.8601
  Training batch 40: loss=0.0392, recon_error=0.0344, vq_loss=0.0048, , perplexity_loss=-4.7387
  Training batch 50: loss=0.0429, recon_error=0.0377, vq_loss=0.0052, , perplexity_loss=-4.8480
  Training batch 60: loss=0.0380, recon_error=0.0327, vq_loss=0.0053, , perplexity_loss=-4.8558
  Training batch 70: loss=0.0379, recon_error=0.0330, vq_loss=0.0049, , perplexity_loss=-4.8345
  Training batch 80: loss=0.0367, recon_error=0.0318, vq_loss=0.0050, , perplexity_loss=-4.8392
  Training batch 90: loss=0.0415, recon_error=0.0364, vq_loss=0.0051, , perplexity_loss=-4.8317
  Validation batch 0: loss=0.0399, recon_error=0.0349, vq_loss=0.0050, perplexity_loss=-4.8293
  Validation batch 10: loss=0.0377, recon_error=0.0326, vq_loss=0.0051, perplexity_loss=-4.7142
Epoch 43, Train Loss: 0.038, Train Reconstruct Loss: 0.033, Train VQ Loss: 0.005, Train Perplexity Loss: -4.856, Dev Loss: 0.040, Dev Reconstruct Loss: 0.035, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.779, 
Training Perplexity: 111.520
cosine_mean_similarity: 0.887
euclidean_mean_distance: 8.024
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3919.0

Epoch 44/50, batches: 98
  Training batch 0: loss=0.0346, recon_error=0.0296, vq_loss=0.0050, , perplexity_loss=-4.9196
  Training batch 10: loss=0.0362, recon_error=0.0313, vq_loss=0.0050, , perplexity_loss=-4.8266
  Training batch 20: loss=0.0398, recon_error=0.0346, vq_loss=0.0051, , perplexity_loss=-4.8841
  Training batch 30: loss=0.0329, recon_error=0.0279, vq_loss=0.0051, , perplexity_loss=-4.9206
  Training batch 40: loss=0.0364, recon_error=0.0314, vq_loss=0.0050, , perplexity_loss=-4.8572
  Training batch 50: loss=0.0418, recon_error=0.0369, vq_loss=0.0049, , perplexity_loss=-4.8451
  Training batch 60: loss=0.0435, recon_error=0.0387, vq_loss=0.0048, , perplexity_loss=-4.7847
  Training batch 70: loss=0.0458, recon_error=0.0407, vq_loss=0.0051, , perplexity_loss=-4.8401
  Training batch 80: loss=0.0389, recon_error=0.0340, vq_loss=0.0048, , perplexity_loss=-4.8348
  Training batch 90: loss=0.0366, recon_error=0.0316, vq_loss=0.0050, , perplexity_loss=-4.9126
  Validation batch 0: loss=0.0397, recon_error=0.0348, vq_loss=0.0050, perplexity_loss=-4.7629
  Validation batch 10: loss=0.0394, recon_error=0.0343, vq_loss=0.0051, perplexity_loss=-4.6788
Epoch 44, Train Loss: 0.039, Train Reconstruct Loss: 0.034, Train VQ Loss: 0.005, Train Perplexity Loss: -4.857, Dev Loss: 0.043, Dev Reconstruct Loss: 0.038, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.714, 
Training Perplexity: 107.641
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.993
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4206.0

Epoch 45/50, batches: 98
  Training batch 0: loss=0.0345, recon_error=0.0295, vq_loss=0.0050, , perplexity_loss=-4.8699
  Training batch 10: loss=0.0346, recon_error=0.0297, vq_loss=0.0049, , perplexity_loss=-4.8882
  Training batch 20: loss=0.0395, recon_error=0.0348, vq_loss=0.0048, , perplexity_loss=-4.7853
  Training batch 30: loss=0.0445, recon_error=0.0395, vq_loss=0.0050, , perplexity_loss=-4.8587
  Training batch 40: loss=0.0369, recon_error=0.0319, vq_loss=0.0051, , perplexity_loss=-4.9202
  Training batch 50: loss=0.0336, recon_error=0.0285, vq_loss=0.0051, , perplexity_loss=-4.9232
  Training batch 60: loss=0.0357, recon_error=0.0307, vq_loss=0.0049, , perplexity_loss=-4.8884
  Training batch 70: loss=0.0372, recon_error=0.0322, vq_loss=0.0050, , perplexity_loss=-4.8771
  Training batch 80: loss=0.0451, recon_error=0.0402, vq_loss=0.0048, , perplexity_loss=-4.8114
  Training batch 90: loss=0.0357, recon_error=0.0308, vq_loss=0.0049, , perplexity_loss=-4.8462
  Validation batch 0: loss=0.0390, recon_error=0.0341, vq_loss=0.0049, perplexity_loss=-4.7374
  Validation batch 10: loss=0.0379, recon_error=0.0329, vq_loss=0.0050, perplexity_loss=-4.6512
Epoch 45, Train Loss: 0.038, Train Reconstruct Loss: 0.033, Train VQ Loss: 0.005, Train Perplexity Loss: -4.858, Dev Loss: 0.040, Dev Reconstruct Loss: 0.035, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.700, 
Training Perplexity: 104.709
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.964
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4171.0

Epoch 46/50, batches: 98
  Training batch 0: loss=0.0382, recon_error=0.0332, vq_loss=0.0050, , perplexity_loss=-4.8803
  Training batch 10: loss=0.0353, recon_error=0.0303, vq_loss=0.0050, , perplexity_loss=-4.9168
  Training batch 20: loss=0.0388, recon_error=0.0339, vq_loss=0.0049, , perplexity_loss=-4.8407
  Training batch 30: loss=0.0376, recon_error=0.0327, vq_loss=0.0049, , perplexity_loss=-4.8783
  Training batch 40: loss=0.0368, recon_error=0.0320, vq_loss=0.0048, , perplexity_loss=-4.8432
  Training batch 50: loss=0.0407, recon_error=0.0357, vq_loss=0.0050, , perplexity_loss=-4.8895
  Training batch 60: loss=0.0464, recon_error=0.0412, vq_loss=0.0052, , perplexity_loss=-4.9360
  Training batch 70: loss=0.0428, recon_error=0.0379, vq_loss=0.0050, , perplexity_loss=-4.8722
  Training batch 80: loss=0.0367, recon_error=0.0320, vq_loss=0.0048, , perplexity_loss=-4.8552
  Training batch 90: loss=0.0362, recon_error=0.0314, vq_loss=0.0047, , perplexity_loss=-4.8180
  Validation batch 0: loss=0.0396, recon_error=0.0347, vq_loss=0.0049, perplexity_loss=-4.7495
  Validation batch 10: loss=0.0387, recon_error=0.0337, vq_loss=0.0050, perplexity_loss=-4.6620
Epoch 46, Train Loss: 0.038, Train Reconstruct Loss: 0.033, Train VQ Loss: 0.005, Train Perplexity Loss: -4.859, Dev Loss: 0.041, Dev Reconstruct Loss: 0.036, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.709, 
Training Perplexity: 105.851
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.934
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4174.0

Epoch 47/50, batches: 98
  Training batch 0: loss=0.0398, recon_error=0.0347, vq_loss=0.0050, , perplexity_loss=-4.9168
  Training batch 10: loss=0.0394, recon_error=0.0346, vq_loss=0.0049, , perplexity_loss=-4.8019
  Training batch 20: loss=0.0431, recon_error=0.0384, vq_loss=0.0047, , perplexity_loss=-4.8185
  Training batch 30: loss=0.0402, recon_error=0.0352, vq_loss=0.0050, , perplexity_loss=-4.8809
  Training batch 40: loss=0.0387, recon_error=0.0336, vq_loss=0.0051, , perplexity_loss=-4.8883
  Training batch 50: loss=0.0385, recon_error=0.0337, vq_loss=0.0048, , perplexity_loss=-4.8820
  Training batch 60: loss=0.0369, recon_error=0.0320, vq_loss=0.0049, , perplexity_loss=-4.8365
  Training batch 70: loss=0.0346, recon_error=0.0296, vq_loss=0.0050, , perplexity_loss=-4.9027
  Training batch 80: loss=0.0369, recon_error=0.0320, vq_loss=0.0050, , perplexity_loss=-4.8806
  Training batch 90: loss=0.0370, recon_error=0.0323, vq_loss=0.0047, , perplexity_loss=-4.7982
  Validation batch 0: loss=0.0407, recon_error=0.0358, vq_loss=0.0048, perplexity_loss=-4.6943
  Validation batch 10: loss=0.0400, recon_error=0.0350, vq_loss=0.0050, perplexity_loss=-4.6198
Epoch 47, Train Loss: 0.039, Train Reconstruct Loss: 0.034, Train VQ Loss: 0.005, Train Perplexity Loss: -4.864, Dev Loss: 0.042, Dev Reconstruct Loss: 0.037, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.672, 
Training Perplexity: 101.471
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.904
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 4363.0

Epoch 48/50, batches: 98
  Training batch 0: loss=0.0342, recon_error=0.0292, vq_loss=0.0050, , perplexity_loss=-4.9348
  Training batch 10: loss=0.0358, recon_error=0.0309, vq_loss=0.0048, , perplexity_loss=-4.9016
  Training batch 20: loss=0.0427, recon_error=0.0376, vq_loss=0.0051, , perplexity_loss=-4.9315
  Training batch 30: loss=0.0346, recon_error=0.0298, vq_loss=0.0048, , perplexity_loss=-4.8432
  Training batch 40: loss=0.0376, recon_error=0.0328, vq_loss=0.0048, , perplexity_loss=-4.8905
  Training batch 50: loss=0.0367, recon_error=0.0320, vq_loss=0.0047, , perplexity_loss=-4.8331
  Training batch 60: loss=0.0330, recon_error=0.0282, vq_loss=0.0048, , perplexity_loss=-4.8383
  Training batch 70: loss=0.0375, recon_error=0.0323, vq_loss=0.0052, , perplexity_loss=-4.9878
  Training batch 80: loss=0.0349, recon_error=0.0300, vq_loss=0.0049, , perplexity_loss=-4.8517
  Training batch 90: loss=0.0390, recon_error=0.0341, vq_loss=0.0049, , perplexity_loss=-4.8738
  Validation batch 0: loss=0.0419, recon_error=0.0371, vq_loss=0.0048, perplexity_loss=-4.8359
  Validation batch 10: loss=0.0383, recon_error=0.0334, vq_loss=0.0049, perplexity_loss=-4.7482
Epoch 48, Train Loss: 0.037, Train Reconstruct Loss: 0.033, Train VQ Loss: 0.005, Train Perplexity Loss: -4.862, Dev Loss: 0.041, Dev Reconstruct Loss: 0.036, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.797, 
Training Perplexity: 115.371
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.874
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3898.0

Epoch 49/50, batches: 98
  Training batch 0: loss=0.0370, recon_error=0.0323, vq_loss=0.0046, , perplexity_loss=-4.8470
  Training batch 10: loss=0.0385, recon_error=0.0336, vq_loss=0.0049, , perplexity_loss=-4.9095
  Training batch 20: loss=0.0329, recon_error=0.0280, vq_loss=0.0049, , perplexity_loss=-4.8695
  Training batch 30: loss=0.0326, recon_error=0.0277, vq_loss=0.0050, , perplexity_loss=-4.8599
  Training batch 40: loss=0.0361, recon_error=0.0311, vq_loss=0.0049, , perplexity_loss=-4.8952
  Training batch 50: loss=0.0358, recon_error=0.0309, vq_loss=0.0049, , perplexity_loss=-4.8010
  Training batch 60: loss=0.0335, recon_error=0.0286, vq_loss=0.0048, , perplexity_loss=-4.8538
  Training batch 70: loss=0.0443, recon_error=0.0395, vq_loss=0.0048, , perplexity_loss=-4.8673
  Training batch 80: loss=0.0364, recon_error=0.0316, vq_loss=0.0048, , perplexity_loss=-4.8704
  Training batch 90: loss=0.0331, recon_error=0.0284, vq_loss=0.0047, , perplexity_loss=-4.8914
  Validation batch 0: loss=0.0377, recon_error=0.0329, vq_loss=0.0048, perplexity_loss=-4.8316
  Validation batch 10: loss=0.0359, recon_error=0.0310, vq_loss=0.0049, perplexity_loss=-4.7292
Epoch 49, Train Loss: 0.036, Train Reconstruct Loss: 0.031, Train VQ Loss: 0.005, Train Perplexity Loss: -4.864, Dev Loss: 0.038, Dev Reconstruct Loss: 0.033, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.792, 
Training Perplexity: 113.201
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.848
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3651.0
Best model updated and saved at epoch 49

Epoch 50/50, batches: 98
  Training batch 0: loss=0.0299, recon_error=0.0251, vq_loss=0.0048, , perplexity_loss=-4.9039
  Training batch 10: loss=0.0357, recon_error=0.0310, vq_loss=0.0047, , perplexity_loss=-4.8750
  Training batch 20: loss=0.0308, recon_error=0.0261, vq_loss=0.0048, , perplexity_loss=-4.8614
  Training batch 30: loss=0.0345, recon_error=0.0298, vq_loss=0.0047, , perplexity_loss=-4.8277
  Training batch 40: loss=0.0401, recon_error=0.0356, vq_loss=0.0045, , perplexity_loss=-4.7907
  Training batch 50: loss=0.0278, recon_error=0.0230, vq_loss=0.0047, , perplexity_loss=-4.9150
  Training batch 60: loss=0.0413, recon_error=0.0365, vq_loss=0.0048, , perplexity_loss=-4.9203
  Training batch 70: loss=0.0488, recon_error=0.0441, vq_loss=0.0047, , perplexity_loss=-4.7886
  Training batch 80: loss=0.0396, recon_error=0.0348, vq_loss=0.0048, , perplexity_loss=-4.9460
  Training batch 90: loss=0.0414, recon_error=0.0366, vq_loss=0.0047, , perplexity_loss=-4.8363
  Validation batch 0: loss=0.0378, recon_error=0.0330, vq_loss=0.0048, perplexity_loss=-4.8138
  Validation batch 10: loss=0.0356, recon_error=0.0307, vq_loss=0.0049, perplexity_loss=-4.7022
Epoch 50, Train Loss: 0.035, Train Reconstruct Loss: 0.030, Train VQ Loss: 0.005, Train Perplexity Loss: -4.864, Dev Loss: 0.037, Dev Reconstruct Loss: 0.032, Dev VQ Loss: 0.005, Dev Perplexity Loss: -4.763, 
Training Perplexity: 110.193
cosine_mean_similarity: 0.887
euclidean_mean_distance: 7.821
Codebook details: 305/400 vectors used
Usage counts - Min: 0.0, Max: 3982.0
Best model updated and saved at epoch 50
===== Running inference with VQC model =====

Performing inference on test data...

Average Inference Perplexity: 87.600

Codebook Usage During Inference:
Active codes: 250/400 (62.50%)
Unused codes: 150
Token to index mapping saved to ../output/eraser-movie/8_12_encoder_temp1_k5_random_K400/token_to_index_map.json
===== Extracting codebook vectors =====
Loading checkpoint from ../output/eraser-movie/8_12_encoder_temp1_k5_random_K400/model.pt
Codebook vectors saved to ../output/eraser-movie/8_12_encoder_temp1_k5_random_K400/codebook_vectors.pt
Number of clusters: 400
Vector dimension: 768
===== Generating explanations for salient tokens =====
Processing complete! Output file saved as: ../output/eraser-movie/8_12_encoder_temp1_k5_random_K400/merged_explanations.csv
===== Analyzing latent concepts =====
Start generating latent concepts_norm
Error in generating word cloud for concept:  ['1998', '99', '1999', '1998', '1998', '1994', '1998', '1999', '1994', '2001', '2000', '2000', '1998', '1996', '1993', '1984', '2000', '1995', '2000', '1995', '1997', '1993', '1985', '2001', '1984', '2000', '1998', '2001', '1999', '2001', '1998', '1992', '1997', '1994', '1996', '1984', '2000', '1999', '1997', '1998', '2001', '1984', '90', '1998', '2001', '1984', '1996', '1999', '1997', '1989', '1992', '1989', '2001', '1985', '1993', '1997', '1999', '1984', '1985', '1999', '1995', '1995', '1998', '1995', '1993', '1992', '1997', '1985', '2001', '80', '1992', '1996', '1978', '1989', '1993', '1999', '1995', '1999', '1995', '1995', '1996', '1993', '80', '1999', '1988', '60', '80', '70', '80', '90', '2000', '1995', '1998']
Error in generating word cloud for concept:  ['-', '-', '-', '-', '-', '', '', '', '-', '-', '', '', '-', '*', '', '', '-', "'", "'", '*', "'", "'", '', '', "'", "'", '', '', '`', '', '', '`', '/', '', '*', '*', ')', '', '/', '"', '"', '*', '*', '`', ')', "'", ')', ')', ')', '!', ')', "'", '`', '?', ':', ';', ';', '+', ')', '=', ';', '*', '!', '?', '!', ':', '"', '"', '"', ',', ':', '?', '?', '!', '!', ';', "'", '"', '"', '"', "'", '?', "'", '.', '`', '*', '"', '=', '"', "'", '?', '!', '`', '(', ')', "'", '"', '`', ':', ';', ')', '?', '?', '.', '--', '.', ';', '`', '`', '?', '.', '', '?', ':', '--', '"', '"', "'", ',', "'", '.', '.', "'", '.', '?', ':', ')', ':', '?', '`', '(', ')', '!', ':', '---', '---', '?', '.', '?', '!', '"', ')', '.', '--', '!', '(', '!', '', '', '', '---', '`', '=', '=', '=', '`', ':', '?', ';', ',', ',', '.', '-', "'", '.', '.', ')', ')', ')', "'", '`', '`', '?', '"', '?', '.', '(', ')', '`', '=', ':', ':', '?', '!', '!', ')', '?', '.', '--', '`', '.', '--', '*', '(', ',', ':', ';', '.', ':', '?', '.', ';', '(', '(', '=', '(', '--', '.', '.', '!', '(', '=', '(', '---', '`', '(', '"', '(', '$', '+', ':', '"', "'", ',', ';', '(', '(', '(', '"', '-', '"', '!', '--', '-', '/', '/', '+', ';', '*', '/', ':', ':', '!', '--', '(']
Error in generating word cloud for concept:  ['80', '80', '80', '1990', '1998', '1999', '1990', '1989', '1993', '2001', '1993', '2001', '2001', '1997', '1993', '1993', '1995', '1997', '1997', '1995', '2000', '1997', '1988', '1995', '2001', '1999', '2001', '1988', '1989', '2001', '1998', '1996', '1997', '1992', '1993', '1998', '2001', '1992', '2001', '1990', '1996', '1990', '1978', '1993', '1994', '1998', '1996', '1997', '1984', '1988', '1978', '1997', '2001', '1995', '1999', '1998', '1998', '1994', '1998', '1996', '1996', '1999', '1992', '1994', '1984', '1998', '1990', '1999', '1995', '1999', '1995', '1992', '1985', '1992', '1996', '1985', '1997', '1994', '2001', '1995', '1997', '1988', '1994', '2000', '1998', '1992', '1994', '1996', '1978', '2001', '1997', '1997', '1992', '1999', '1997', '1994', '1997', '1992', '1996', '1978', '2000', '2001']
===== All processing complete =====
